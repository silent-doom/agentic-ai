{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6Px1FIhOP8GBoelwjmGX+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silent-doom/agentic-ai/blob/feature%2Feditor-agent/Faizan_Editor_agent_v2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "8cWqUltkABXy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "import sys\n",
        "import subprocess\n",
        "import urllib.request\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# 0. LIGHTWEIGHT SETUP (No MediaPipe/TensorFlow)\n",
        "# =================================================================\n",
        "\n",
        "def install_lightweight_dependencies():\n",
        "    \"\"\"Installs only the necessary, stable libraries.\"\"\"\n",
        "    try:\n",
        "        import moviepy\n",
        "        import yt_dlp\n",
        "        import whisper\n",
        "    except ImportError:\n",
        "        print(\"üì¶ Installing lightweight dependencies...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "            \"moviepy==1.0.3\", \"yt-dlp\", \"git+https://github.com/openai/whisper.git\"])\n",
        "\n",
        "        # System deps for MoviePy\n",
        "        subprocess.run(\"apt update -qq && apt install -qq imagemagick\", shell=True, check=False)\n",
        "        subprocess.run(\"sed -i 's/none/read,write/' /etc/ImageMagick-6/policy.xml\", shell=True, check=False)\n",
        "\n",
        "install_lightweight_dependencies()"
      ],
      "metadata": {
        "id": "kt6aAStYAM2F"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import cv2\n",
        "import numpy as np\n",
        "import yt_dlp\n",
        "import whisper\n",
        "from moviepy.editor import VideoFileClip, TextClip, CompositeVideoClip\n",
        "from moviepy.video.fx.all import crop\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "24n6-dTHwOP9"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# 1. CONFIGURATION & ROBUST PATH DISCOVERY\n",
        "# =================================================================\n",
        "\n",
        "# Padding to prevent abrupt starts/ends (seconds)\n",
        "TIME_PADDING = 1.5\n",
        "\n",
        "def get_robust_paths():\n",
        "    \"\"\"\n",
        "    Attempts to locate the AI_Transcripts folder across different mount points.\n",
        "    Returns (TRANSCRIPT_FOLDER, INPUT_PLAN_FILE, OUTPUT_FOLDER)\n",
        "    \"\"\"\n",
        "    possible_bases = [\n",
        "        \"/content/drive/My Drive/AI_Transcripts\",\n",
        "        \"/content/gdrive/MyDrive/AI_Transcripts\",\n",
        "        \"/content/gdrive/My Drive/AI_Transcripts\",\n",
        "        \"/content/drive/MyDrive/AI_Transcripts\"\n",
        "    ]\n",
        "\n",
        "    for base in possible_bases:\n",
        "        if os.path.exists(base):\n",
        "            os.listdir(base) # Force refresh\n",
        "            plan_file = os.path.join(base, 'viral_clip_plan_new.json')\n",
        "            if os.path.exists(plan_file):\n",
        "                print(f\"‚úÖ Found planning data at: {base}\")\n",
        "                return base, plan_file, os.path.join(base, 'final_shorts')\n",
        "\n",
        "    default_base = \"/content/drive/My Drive/AI_Transcripts\"\n",
        "    return default_base, os.path.join(default_base, 'viral_clip_plan_new.json'), os.path.join(default_base, 'final_shorts')\n",
        "\n",
        "# Haar Cascade for Face Detection\n",
        "HAAR_URL = \"https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\"\n",
        "HAAR_PATH = \"/content/haarcascade_frontalface_default.xml\"\n",
        "\n",
        "# Video URL (Ideally read this from the JSON plan in future updates)\n",
        "YOUTUBE_URL = \"https://www.youtube.com/watch?v=HAnw168huqA\""
      ],
      "metadata": {
        "id": "MjWKp87QxkFZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# 2. UTILITY: SEGMENT DOWNLOAD (Partial)\n",
        "# =================================================================\n",
        "\n",
        "def _parse_time_to_seconds(time_str):\n",
        "    \"\"\"Handles HH:MM:SS or MM:SS.\"\"\"\n",
        "    parts = list(map(int, time_str.split(':')))\n",
        "    if len(parts) == 3:\n",
        "        return parts[0] * 3600 + parts[1] * 60 + parts[2]\n",
        "    return parts[0] * 60 + parts[1]\n",
        "\n",
        "def download_segment(url, start_time_str, end_time_str, output_path, cid):\n",
        "    \"\"\"Downloads partial video segment using yt-dlp to save bandwidth.\"\"\"\n",
        "    if os.path.exists(output_path):\n",
        "        return output_path\n",
        "\n",
        "    # Apply padding to capture the 'essence' of the speech\n",
        "    start_s = max(0, _parse_time_to_seconds(start_time_str) - TIME_PADDING)\n",
        "    end_s = _parse_time_to_seconds(end_time_str) + TIME_PADDING\n",
        "\n",
        "    print(f\"‚¨áÔ∏è [Clip {cid}] Downloading segment: {start_s}s to {end_s}s...\", flush=True)\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
        "        'outtmpl': output_path,\n",
        "        'quiet': True,\n",
        "        'download_ranges': lambda _, __: [{'start_time': start_s, 'end_time': end_s}],\n",
        "        'force_keyframes_at_cuts': True,\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "        return output_path\n",
        "    except Exception as e:\n",
        "        print(f\"üî¥ [Clip {cid}] Download error: {e}\", flush=True)\n",
        "        return None"
      ],
      "metadata": {
        "id": "ff9BOjl6yXYj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# 3. LIGHTWEIGHT FACE TRACKING\n",
        "# =================================================================\n",
        "\n",
        "def setup_face_detector():\n",
        "    if not os.path.exists(HAAR_PATH):\n",
        "        urllib.request.urlretrieve(HAAR_URL, HAAR_PATH)\n",
        "    return cv2.CascadeClassifier(HAAR_PATH)\n",
        "\n",
        "def detect_face_x_center(frame, face_cascade):\n",
        "    \"\"\"Detects face using OpenCV to avoid numpy/tensorflow conflicts.\"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    if len(faces) == 0: return None\n",
        "    x, y, w, h = max(faces, key=lambda f: f[2] * f[3])\n",
        "    return (x + (w / 2)) / frame.shape[1]\n",
        "\n",
        "def vertical_crop_smart(clip, cid):\n",
        "    \"\"\"Crops to 9:16 using Median face position for stability.\"\"\"\n",
        "    print(f\"ü§ñ [Clip {cid}] Tracking face for smart crop...\", flush=True)\n",
        "    face_cascade = setup_face_detector()\n",
        "    face_x_positions = []\n",
        "\n",
        "    # Sample every 0.5s for precise tracking\n",
        "    duration = int(clip.duration)\n",
        "    for t in np.arange(0, duration, 0.5):\n",
        "        try:\n",
        "            x_pos = detect_face_x_center(clip.get_frame(t), face_cascade)\n",
        "            if x_pos: face_x_positions.append(x_pos)\n",
        "        except: pass\n",
        "\n",
        "    # MEDIAN filters out background noise or transient face detections\n",
        "    avg_x = np.median(face_x_positions) if face_x_positions else 0.5\n",
        "\n",
        "    w, h = clip.size\n",
        "    new_width = h * (9/16)\n",
        "    x1 = int((avg_x * w) - (new_width / 2))\n",
        "    x1 = max(0, min(x1, w - int(new_width))) # Keep in frame\n",
        "\n",
        "    cropped = crop(clip, x1=x1, y1=0, width=int(new_width), height=h)\n",
        "    return cropped.resize(height=1920)\n"
      ],
      "metadata": {
        "id": "C33weODVyiOp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# 4. CAPTIONING ENGINE (Hardware Aware)\n",
        "# =================================================================\n",
        "\n",
        "def generate_dynamic_captions(video_clip, clip_id, device):\n",
        "    \"\"\"Generates Whisper captions using local model.\"\"\"\n",
        "    print(f\"üìù [Clip {clip_id}] Generating captions...\", flush=True)\n",
        "    audio_path = f\"/content/temp_audio_{clip_id}.wav\"\n",
        "    video_clip.audio.write_audiofile(audio_path, logger=None)\n",
        "\n",
        "    # FP16 is only for CUDA\n",
        "    use_fp16 = (device == \"cuda\")\n",
        "    model = whisper.load_model(\"base\", device=device)\n",
        "\n",
        "    result = model.transcribe(audio_path, word_timestamps=True, fp16=use_fp16)\n",
        "\n",
        "    caption_clips = []\n",
        "    for segment in result['segments']:\n",
        "        for word in segment.get('words', []):\n",
        "            txt = word['word'].strip().upper()\n",
        "            start, end = word['start'], word['end']\n",
        "            if end - start < 0.05: continue\n",
        "\n",
        "            # Karaoke-style yellow captions\n",
        "            c = (TextClip(txt, fontsize=95, color='yellow', font='Arial-Bold', stroke_color='black', stroke_width=2)\n",
        "                 .set_position(('center', 0.8), relative=True)\n",
        "                 .set_start(start)\n",
        "                 .set_duration(end - start))\n",
        "            caption_clips.append(c)\n",
        "\n",
        "    if os.path.exists(audio_path): os.remove(audio_path)\n",
        "    return caption_clips"
      ],
      "metadata": {
        "id": "XNNkOmpkymJb"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# 5. WORKER LOGIC\n",
        "# =================================================================\n",
        "\n",
        "def process_single_clip(clip_data, output_folder, device):\n",
        "    \"\"\"Full processing pipeline for a single segment.\"\"\"\n",
        "    cid = clip_data['clip_id']\n",
        "    hook = clip_data['viral_hook']\n",
        "    temp_path = f\"/content/raw_{cid}.mp4\"\n",
        "    clean_name = re.sub(r'[^A-Za-z0-9]', '', hook[:15])\n",
        "    final_path = os.path.join(output_folder, f\"Short_{cid}_{clean_name}.mp4\")\n",
        "\n",
        "    if not download_segment(YOUTUBE_URL, clip_data['start_time'], clip_data['end_time'], temp_path, cid):\n",
        "        return f\"Clip {cid} failed at download.\"\n",
        "\n",
        "    try:\n",
        "        with VideoFileClip(temp_path) as raw:\n",
        "            # Reframe\n",
        "            vertical = vertical_crop_smart(raw, cid)\n",
        "            # Transcribe & Caption\n",
        "            captions = generate_dynamic_captions(vertical, cid, device)\n",
        "            # Composite\n",
        "            final = CompositeVideoClip([vertical] + captions)\n",
        "\n",
        "            print(f\"üíæ [Clip {cid}] Rendering final file...\", flush=True)\n",
        "            # Use 'ultrafast' for speed, multi-threading enabled\n",
        "            final.write_videofile(final_path, codec='libx264', audio_codec='aac',\n",
        "                                 fps=24, preset='ultrafast', threads=4, logger=None)\n",
        "\n",
        "        return f\"‚úÖ Clip {cid} saved to Drive.\"\n",
        "    except Exception as e:\n",
        "        return f\"üî¥ Error in clip {cid}: {e}\"\n",
        "    finally:\n",
        "        if os.path.exists(temp_path): os.remove(temp_path)"
      ],
      "metadata": {
        "id": "nqSVk6zFJshY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =================================================================\n",
        "# 6. MAIN RUNNER (Auto-Scaling)\n",
        "# =================================================================\n",
        "\n",
        "def run_editor_agent():\n",
        "    # 1. Mount Drive\n",
        "    if not os.path.exists(\"/content/drive\"):\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive')\n",
        "\n",
        "    # 2. Hardware Detection\n",
        "    gpu_check = subprocess.run(\"nvidia-smi\", shell=True, capture_output=True)\n",
        "    if gpu_check.returncode == 0:\n",
        "        device = \"cuda\"\n",
        "        num_workers = 4 # Parallel workers for GPU\n",
        "        print(\"üöÄ CUDA device detected! Scaling up to 4 parallel workers.\")\n",
        "    else:\n",
        "        device = \"cpu\"\n",
        "        num_workers = 2 # Safely fallback to 2 workers for CPU\n",
        "        print(\"üê¢ No GPU found. Falling back to 2 workers on CPU.\")\n",
        "\n",
        "    # 3. Path Discovery\n",
        "    TRANS_DIR, PLAN_FILE, OUT_DIR = get_robust_paths()\n",
        "    os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(PLAN_FILE):\n",
        "        print(f\"üî¥ Error: Planning file not found at {PLAN_FILE}\")\n",
        "        return\n",
        "\n",
        "    with open(PLAN_FILE, 'r') as f:\n",
        "        plan = json.load(f)\n",
        "\n",
        "    clips = plan.get('viral_clips', [])\n",
        "    print(f\"üé¨ Scheduled {len(clips)} clips for processing. Please wait for logs below...\")\n",
        "\n",
        "    # 4. Parallel Execution with Future Tracking for better visibility\n",
        "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Create a dictionary to map futures to clip IDs for tracking\n",
        "        future_to_clip = {executor.submit(process_single_clip, clip, OUT_DIR, device): clip['clip_id'] for clip in clips}\n",
        "\n",
        "        for future in as_completed(future_to_clip):\n",
        "            clip_id = future_to_clip[future]\n",
        "            try:\n",
        "                data = future.result()\n",
        "                print(f\"üèÅ Update: {data}\", flush=True)\n",
        "            except Exception as exc:\n",
        "                print(f\"üî¥ Clip {clip_id} generated an exception: {exc}\", flush=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_editor_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QfdONnRyo6M",
        "outputId": "506291d8-2443-43c5-eb95-c2c8e4d41df9"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ CUDA device detected! Scaling up to 4 parallel workers.\n",
            "‚úÖ Found planning data at: /content/drive/My Drive/AI_Transcripts\n",
            "üé¨ Scheduled 5 clips for processing. Please wait for logs below...\n",
            "‚¨áÔ∏è [Clip 1] Downloading segment: 1558.5s to 1651.5s...\n",
            "‚¨áÔ∏è [Clip 2] Downloading segment: 418.5s to 476.5s...\n",
            "ü§ñ [Clip 3] Tracking face for smart crop...\n",
            "ü§ñ [Clip 4] Tracking face for smart crop...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one\n",
            "WARNING: [youtube] HAnw168huqA: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one\n",
            "WARNING: [youtube] HAnw168huqA: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] HAnw168huqA: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] HAnw168huqA: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Success: /content/drive/My Drive/AI_Transcripts/final_shorts/Short_4_StopsayingNo.mp4\n",
            "üé¨ Starting Clip 5...\n",
            "‚úÖ Success: /content/drive/My Drive/AI_Transcripts/final_shorts/Short_3_Didyoumissth.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/raw_4.mp4, 2764800 bytes wanted but 0 bytes read,at frame 1/3717, at time 0.02/62.01 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è [Clip 5] Downloading segment: 2338.5s to 2401.5s...\n",
            "üèÅ Update: üî¥ Error in clip 4: MoviePy error: failed to read the first frame of video file /content/raw_4.mp4. That might mean that the file is corrupted. That may also mean that you are using a deprecated version of FFMPEG. On Ubuntu/Debian for instance the version in the repos is deprecated. Please update to a recent version from the website.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] No supported JavaScript runtime could be found. Only deno is enabled by default; to use another runtime add  --js-runtimes RUNTIME[:PATH]  to your command/config. YouTube extraction without a JS runtime has been deprecated, and some formats may be missing. See  https://github.com/yt-dlp/yt-dlp/wiki/EJS  for details on installing one\n",
            "WARNING: [youtube] HAnw168huqA: Some web_safari client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING: [youtube] HAnw168huqA: Some web client https formats have been skipped as they are missing a url. YouTube is forcing SABR streaming for this client. See  https://github.com/yt-dlp/yt-dlp/issues/12482  for more details\n",
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/raw_3.mp4, 2764800 bytes wanted but 0 bytes read,at frame 1/4618, at time 0.02/77.03 sec. Using the last valid frame instead.\n",
            "  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üèÅ Update: üî¥ Error in clip 3: MoviePy error: failed to read the first frame of video file /content/raw_3.mp4. That might mean that the file is corrupted. That may also mean that you are using a deprecated version of FFMPEG. On Ubuntu/Debian for instance the version in the repos is deprecated. Please update to a recent version from the website.\n",
            "ü§ñ [Clip 2] Tracking face for smart crop...\n",
            "üìù [Clip 2] Generating captions...\n",
            "üíæ [Clip 2] Rendering final file...\n",
            "ü§ñ [Clip 5] Tracking face for smart crop...\n",
            "üèÅ Update: ‚úÖ Clip 2 saved to Drive.\n",
            "üìù [Clip 5] Generating captions...\n",
            "üíæ [Clip 5] Rendering final file...\n",
            "ü§ñ [Clip 1] Tracking face for smart crop...\n",
            "üèÅ Update: ‚úÖ Clip 5 saved to Drive.\n",
            "üìù [Clip 1] Generating captions...\n",
            "üíæ [Clip 1] Rendering final file...\n",
            "üèÅ Update: ‚úÖ Clip 1 saved to Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6CFIbpxyzp1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}